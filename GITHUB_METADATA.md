# GitHub Repository Metadata

## Description
**PromptCache**: Cut LLM costs by 80% and reduce latency to sub-second speeds with intelligent semantic caching. Drop-in OpenAI replacement written in Go.

## Tags
`go`, `llm`, `openai`, `cache`, `semantic-search`, `vector-database`, `rag`, `ai`, `performance`, `middleware`, `cost-optimization`, `badgerdb`
